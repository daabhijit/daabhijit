# -*- coding: utf-8 -*-
"""assignment:German Credit Data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kGC5Y2AMtUbjYt0iazgbIGuCEkmu4Hwe

# IMPORTING LIBRARIES
"""

from __future__ import print_function
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report
from sklearn import metrics
from sklearn import tree
import warnings
warnings.filterwarnings('ignore')

"""#READ DATASET"""

df=pd.read_csv('/content/german_credit_data.csv')

df.head()

df.shape

df.columns

column_mapping = {
    "laufkont": "Duration in month",
    "laufzeit": "Credit history",
    "moral": "Purpose",
    "verw": "Credit amount",
    "hoehe": "Savings account",
    "sparkont": "Present employment since",
    "beszeit": "Installment rate in percentage of disposable income",
    "rate": "Personal Status Sex",
    "famges": "Other debtors",
    "buerge": "Present residence since",
    "verm": "Property",
    "alter": "Age in years",
    "weitkred": "Other installment plans",
    "wohn": "Housing",
    "bishkred": "Number of existing credits at this bank",
    "beruf": "Job",
    "pers": "Number of dependents",
    "telef": "Telephone",
    "gastarb": "Foreign worker",
    "kredit": "credit risk"
}

df.rename(columns=column_mapping, inplace=True)

df.head()

df.columns

df.isnull().sum()

df['credit risk'].unique()

df.dtypes

df['credit risk'].value_counts()

"""#APPLYING KNN,SVM,LOGISTIC REGRESSION,DECISION TREE,RANDOM FOREST"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

X = df.drop('credit risk', axis=1)
y = df['credit risk']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

classifiers = {
    'Logistic Regression': LogisticRegression(),
    'K Nearest Neighbors': KNeighborsClassifier(),
    'Support Vector Machine': SVC(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier()
}
param_grids = {
    'Logistic Regression': {'C': [0.1, 1.0, 10.0]},
    'K Nearest Neighbors': {'n_neighbors': [3, 5, 7]},
    'Support Vector Machine': {'C': [0.1, 1.0, 10.0], 'gamma': ['scale', 'auto']},
    'Decision Tree': {'max_depth': [3, 5, 7]},
    'Random Forest': {'n_estimators': [50, 100, 200]}
}
from sklearn.model_selection import GridSearchCV
best_estimators = {}
for clf_name, clf in classifiers.items():
    grid_search = GridSearchCV(clf, param_grids[clf_name], cv=5, scoring='accuracy')
    grid_search.fit(X_train, y_train)
    best_estimators[clf_name] = grid_search.best_estimator_

for clf_name, clf in best_estimators.items():
    accuracy = clf.score(X_test, y_test)
    print(f'{clf_name}:')
    print(f'Best parameters: {clf.get_params()}')
    print(f'Accuracy: {accuracy}')
    print()

trained_models = {}
for clf_name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    trained_models[clf_name] = clf

"""#APPLYING CROSS-VALIDATION"""

from sklearn.model_selection import cross_val_score

for clf_name, clf in trained_models.items():
    scores = cross_val_score(clf, X, y, cv=5)
    print(f'{clf_name}:')
    print(f'Cross-validation scores: {scores}')
    print(f'Average cross-validation score: {np.mean(scores)}')
    print()

"""#APPLYING K-FOLD CROSS-VALIDATION"""

from sklearn.model_selection import KFold

k_folds = 3

kf = KFold(n_splits=k_folds)
for clf_name, clf in trained_models.items():
    scores = cross_val_score(clf, X, y, cv=kf)
    print(f'{clf_name}:')
    print(f'Cross-validation scores: {scores}')
    print(f'Average cross-validation score: {np.mean(scores)}')
    print()

from sklearn.model_selection import KFold
k_folds = 5
kf = KFold(n_splits=k_folds)
for clf_name, clf in trained_models.items():
    scores = cross_val_score(clf, X, y, cv=kf)
    print(f'{clf_name}:')
    print(f'Cross-validation scores: {scores}')
    print(f'Average cross-validation score: {np.mean(scores)}')
    print()

from sklearn.model_selection import KFold

k_folds = 6

kf = KFold(n_splits=k_folds)

for clf_name, clf in trained_models.items():
    scores = cross_val_score(clf, X, y, cv=kf)
    print(f'{clf_name}:')
    print(f'Cross-validation scores: {scores}')
    print(f'Average cross-validation score: {np.mean(scores)}')
    print()

""" Observations based on cross-validation results:
 - Logistic Regression and Support Vector Machine show improvements in performance after cross-validation, indicating better generalization to unseen data.
 - K Nearest Neighbors, Decision Tree, and Random Forest show decreases in performance after cross-validation, suggesting potential issues with overfitting or generalization to unseen data.
 - These observations highlight the importance of using cross-validation techniques to assess model performance accurately and to avoid overfitting.

"""

import matplotlib.pyplot as plt
k_values = [1, 3, 5, 7, 9, 11, 13, 15]

mean_accuracies = []

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, X_scaled, y, cv=5, scoring='accuracy')
    mean_accuracy = scores.mean()
    mean_accuracies.append(mean_accuracy)

plt.figure(figsize=(10, 6))
plt.plot(k_values, mean_accuracies, marker='o', linestyle='-')
plt.title('Mean Accuracy vs. Number of Neighbors (K)')
plt.xlabel('Number of Neighbors (K)')
plt.ylabel('Mean Accuracy')
plt.grid(True)
plt.show()

foreign_worker_distribution = df['Foreign worker'].value_counts(normalize=True)
print("Distribution of 'Foreign worker':")
print(foreign_worker_distribution)

personal_status_sex_distribution = df['Personal Status Sex'].value_counts(normalize=True)
print("\nDistribution of 'Personal Status Sex':")
print(personal_status_sex_distribution)

import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

foreign_worker_distribution.plot(kind='bar', ax=axes[0], color='skyblue')
axes[0].set_title('Distribution of Foreign Worker')
axes[0].set_ylabel('Percentage')
axes[0].set_xlabel('Foreign Worker')
axes[0].set_xticklabels(['No', 'Yes'], rotation=0)

personal_status_sex_distribution.plot(kind='bar', ax=axes[1], color='lightgreen')
axes[1].set_title('Distribution of Personal Status Sex')
axes[1].set_ylabel('Percentage')
axes[1].set_xlabel('Personal Status Sex')

plt.tight_layout()
plt.show()

df.drop(columns=['Personal Status Sex','Foreign worker'],inplace=True)

df.head()

df.columns

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

X = df.drop('credit risk', axis=1)
y = df['credit risk']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)


X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

classifiers = {
    'Logistic Regression': LogisticRegression(),
    'K Nearest Neighbors': KNeighborsClassifier(),
    'Support Vector Machine': SVC(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier()
}

param_grids = {
    'Logistic Regression': {'C': [0.1, 1.0, 10.0]},
    'K Nearest Neighbors': {'n_neighbors': [3, 5, 7]},
    'Support Vector Machine': {'C': [0.1, 1.0, 10.0], 'gamma': ['scale', 'auto']},
    'Decision Tree': {'max_depth': [3, 5, 7]},
    'Random Forest': {'n_estimators': [50, 100, 200]}
}
from sklearn.model_selection import GridSearchCV

best_estimators = {}
for clf_name, clf in classifiers.items():
    grid_search = GridSearchCV(clf, param_grids[clf_name], cv=5, scoring='accuracy')
    grid_search.fit(X_train, y_train)
    best_estimators[clf_name] = grid_search.best_estimator_

for clf_name, clf in best_estimators.items():
    accuracy = clf.score(X_test, y_test)
    print(f'{clf_name}:')
    print(f'Best parameters: {clf.get_params()}')
    print(f'Accuracy: {accuracy}')
    print()
trained_models = {}
for clf_name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    trained_models[clf_name] = clf

""" # Before removing 'personal_status_sex' and 'foreign_worker' attributes:
 - Logistic Regression: Accuracy = 0.75
 - K Nearest Neighbors: Accuracy = 0.77
 - Support Vector Machine: Accuracy = 0.74
 - Decision Tree: Accuracy = 0.685
 - Random Forest: Accuracy = 0.75

 # After removing 'personal_status_sex' and 'foreign_worker' attributes:
 - Logistic Regression: Accuracy = 0.745
 - K Nearest Neighbors: Accuracy = 0.725
 - Support Vector Machine: Accuracy = 0.75
 - Decision Tree: Accuracy = 0.685
 - Random Forest: Accuracy = 0.755

 # Analysis:
 - For Logistic Regression and K Nearest Neighbors, there is a slight decrease in accuracy after removing the attributes.
 - For Support Vector Machine and Random Forest, the accuracy remains similar before and after removing the attributes.
 - For Decision Tree, the accuracy remains unchanged before and after removing the attributes.

"""

from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
model = RandomForestClassifier()
model.fit(X_train, y_train)

importances = model.feature_importances_

indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
plt.title("Feature Importances")
plt.bar(range(X_train.shape[1]), importances[indices], align="center")
plt.xticks(range(X_train.shape[1]), indices)
plt.xlabel("Feature Index")
plt.ylabel("Importance Score")
plt.show()

"""# MODEL ACCURACY"""

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X_scaled)

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)

model = LogisticRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

"""# PREVIOUS MODEL VS PRUNED MODEL (ACCURACY)"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

pruned_dt_classifier = DecisionTreeClassifier(random_state=42, ccp_alpha=0.01)
pruned_dt_classifier.fit(X_train, y_train)

y_pred_pruned_dt = pruned_dt_classifier.predict(X_test)
accuracy_pruned_dt = accuracy_score(y_test, y_pred_pruned_dt)
print("Pruned Decision Tree Accuracy:", accuracy_pruned_dt)

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score, train_test_split
import numpy as np

dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

def prune_tree(tree, X_val, y_val):
    depth = tree.get_depth()

    for d in range(depth, 0, -1):

        nodes = [n for n in range(tree.tree_.node_count) if tree.tree_.children_left[n] != -1 and tree.tree_.children_right[n] != -1 and tree.tree_.threshold[n] != -2 and tree.tree_.impurity[n] < d]
        for node in nodes:
            tree.tree_.threshold[node] = -2
            score = cross_val_score(tree, X_val, y_val, cv=3).mean()

            if score >= cross_val_score(tree, X_train, y_train, cv=3).mean():
                tree.tree_.threshold[node] = -2
            else:
                tree.tree_.threshold[node] = tree.tree_.threshold[node]
    return tree


X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

pruned_tree = prune_tree(dt, X_val_split, y_val_split)

scores = cross_val_score(pruned_tree, X_train, y_train, cv=5)
print(f'Pruned Decision Tree Cross-validation scores: {scores}')
print(f'Pruned Decision Tree Average cross-validation score: {np.mean(scores)}')

unpruned_scores = cross_val_score(dt, X_train, y_train, cv=5)
print(f'Unpruned Decision Tree Cross-validation scores: {unpruned_scores}')
print(f'Unpruned Decision Tree Average cross-validation score: {np.mean(unpruned_scores)}')

"""#Previous model accuracy vs Pruned model accuracy

"""

print("Logistic Regression Accuracy with PCA:", accuracy)
print("Comparison:")
print("- Logistic Regression Accuracy with PCA:", accuracy)
print("- Pruned Decision Tree Accuracy:", accuracy_pruned_dt)